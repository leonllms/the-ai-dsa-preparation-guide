# Tokenizer

## 1ï¸âƒ£  What is a **tokenizer**?

In naturalâ€‘language processing (NLP) a *tokenizer* is the component that turns raw text (a string of characters) into a sequence of **tokens** â€“ the atomic units a model consumes.  
Tokens can be:

| Token type | Example (sentence: â€œI love NLP!â€) |
|------------|-----------------------------------|
| **Wordâ€‘level** | `["I", "love", "NLP", "!"]` |
| **Characterâ€‘level** | `["I", " ", "l", "o", "v", "e", â€¦]` |
| **Subâ€‘word** (e.g. BPE, WordPiece, Unigram) | `["I", "love", "NL", "##P", "!"]` |
| **Byteâ€‘level** (e.g. GPTâ€‘2) | `["Ä I", "Ä love", "Ä N", "L", "P", "!"]` |

The tokenizer does **more than just split on spaces**:

1. **Normalisation** â€“ lowerâ€‘casing, Unicode NFKC/NFKD, stripping accents, etc.  
2. **Preâ€‘processing** â€“ adding special markers (e.g. `Ä ` for a leading space in GPTâ€‘2) or handling punctuation.  
3. **Vocabulary lookup** â€“ mapping each token to an integer ID (the *vocab*).  
4. **Handling unknowns** â€“ using a fallback token like `<unk>` or breaking a word into smaller subâ€‘words.  
5. **Postâ€‘processing** â€“ adding startâ€‘/endâ€‘ofâ€‘sentence tokens, padding, truncation.

A good tokenizer is **fast**, **deterministic**, and **robust** to the many quirks of human language (emoji, diacritics, mixed scripts, etc.).

---

## 2ï¸âƒ£  How does **Byteâ€‘Pair Encoding (BPE)** work?

BPE is a **subâ€‘word tokenisation algorithm** originally invented for data compression (Gage, 1994) and later adapted for NLP (Sennrich etâ€¯al., 2015).  
The idea is simple:

1. **Start** with a vocabulary that contains every *character* (or byte) that appears in the training corpus.  
2. **Count** all adjacent symbol pairs (e.g. `("a","b")`, `("b","c")`, â€¦) across the whole corpus.  
3. **Pick** the most frequent pair and **merge** it into a new symbol (e.g. `ab`).  
4. **Add** the new symbol to the vocabulary and repeat stepsâ€¯2â€‘3 **N** times (or until a target vocab size is reached).

After training, any word can be **greedily** segmented into the longest possible symbols from the learned vocabulary. This yields a compact, languageâ€‘agnostic set of subâ€‘words that can represent rare or outâ€‘ofâ€‘vocab words by breaking them into known pieces.

### Visual example

| Iteration | Most frequent pair | Merge â†’ New Symbol | Example word â€œlowestâ€ after merge |
|-----------|-------------------|-------------------|-----------------------------------|
| 0 (init)  | â€“                 | â€“                 | `l o w e s t`                     |
| 1         | (`l`,`o`)         | `lo`              | `lo w e s t`                      |
| 2         | (`e`,`s`)         | `es`              | `lo w es t`                       |
| 3         | (`w`,`es`)        | `wes`             | `lo wes t`                        |
| 4         | (`lo`,`wes`)      | `lowes`           | `lowes t`                         |
| 5         | (`lowes`,`t`)     | `lowest`          | `lowest` (now a single token)    |

The final vocab might contain `["l","o","w","e","s","t","lo","es","wes","lowes","lowest", â€¦]`.

---

## 3ï¸âƒ£  Schoolâ€‘book (educational) implementation

Below is a **minimal, pureâ€‘Python** implementation that:

* builds a BPE vocab from a list of training sentences,
* merges the most frequent pair `num_merges` times,
* provides `encode` (segment a word) and `decode` (reâ€‘assemble tokens).

> **Note** â€“ This code is intentionally simple for learning purposes.  
> It does **not** handle Unicode normalisation, special tokens, or large corpora efficiently.

```python
# --------------------------------------------------------------
# 1ï¸âƒ£  Helper utilities
# --------------------------------------------------------------
import re
from collections import Counter, defaultdict
from typing import List, Tuple, Dict

def get_initial_vocab(corpus: List[str]) -> Tuple[Dict[Tuple[str, ...], int], List[List[Tuple[str, ...]]]]:
    """
    Turn each word into a tuple of characters + a special endâ€‘ofâ€‘word marker.
    Returns:
        vocab   â€“ mapping from symbol tuple -> frequency (initially char counts)
        tokenized_corpus â€“ list of words represented as list of symbol tuples
    """
    tokenized = []
    vocab = Counter()
    for line in corpus:
        for word in line.strip().split():
            # Append </w> to mark word boundary (standard BPE practice)
            symbols = tuple(word) + ("</w>",)
            tokenized.append(list(symbols))
            vocab.update([symbols])
    return vocab, tokenized

def get_pair_frequencies(tokenized: List[List[Tuple[str, ...]]]) -> Counter:
    """Count all adjacent symbol pairs in the current tokenisation."""
    pairs = Counter()
    for word in tokenized:
        # word is a list of symbols (each symbol is a tuple of chars)
        for i in range(len(word) - 1):
            pairs[(word[i], word[i + 1])] += 1
    return pairs

def merge_pair(pair: Tuple[Tuple[str, ...], Tuple[str, ...]],
               tokenized: List[List[Tuple[str, ...]]]) -> List[List[Tuple[str, ...]]]:
    """Replace all occurrences of `pair` with its merged symbol."""
    merged = []
    bigram = re.escape(' '.join(pair))
    pattern = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')  # not needed for list version, kept for clarity

    for word in tokenized:
        i = 0
        new_word = []
        while i < len(word):
            # If the next two symbols match the pair, merge them
            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                new_word.append(word[i] + word[i + 1])  # tuple concatenation
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        merged.append(new_word)
    return merged

# --------------------------------------------------------------
# 2ï¸âƒ£  BPE trainer
# --------------------------------------------------------------
def train_bpe(corpus: List[str], num_merges: int = 1000) -> Tuple[Dict[str, int], List[Tuple[str, ...]]]:
    """
    Train a BPE tokenizer.
    Returns:
        vocab_dict â€“ mapping token string â†’ id (int)
        merges     â€“ list of merge operations in order (for decoding)
    """
    # 1ï¸âƒ£ Initialise tokenised corpus (list of symbol tuples)
    _, tokenized = get_initial_vocab(corpus)

    merges = []                     # keep the order of merges for later use
    for i in range(num_merges):
        pair_freqs = get_pair_frequencies(tokenized)
        if not pair_freqs:
            break
        most_frequent = pair_freqs.most_common(1)[0][0]   # tuple of two symbols
        merges.append(most_frequent)

        # 2ï¸âƒ£ Merge the most frequent pair everywhere
        tokenized = merge_pair(most_frequent, tokenized)

    # Build final vocab (string representation of each symbol)
    vocab = {}
    idx = 0
    for word in tokenized:
        for sym in word:
            token = ''.join(sym)          # e.g. ('l','o') â†’ "lo"
            if token not in vocab:
                vocab[token] = idx
                idx += 1
    # Add the special unknown token
    vocab["<unk>"] = idx
    return vocab, merges

# --------------------------------------------------------------
# 3ï¸âƒ£  Encoding / Decoding utilities
# --------------------------------------------------------------
def encode_word(word: str, merges: List[Tuple[Tuple[str, ...], Tuple[str, ...]]], vocab: Dict[str, int]) -> List[int]:
    """
    Greedy BPE segmentation of a single word using the learned merges.
    """
    # start with characters + </w>
    symbols = [tuple(c) for c in word] + [("</w>",)]

    # Apply merges in the same order as training
    for merge in merges:
        i = 0
        while i < len(symbols) - 1:
            if (symbols[i], symbols[i + 1]) == merge:
                symbols[i] = symbols[i] + symbols[i + 1]   # merge
                del symbols[i + 1]
            else:
                i += 1

    # Convert to string tokens and look up IDs
    ids = []
    for sym in symbols:
        token = ''.join(sym)
        ids.append(vocab.get(token, vocab["<unk>"]))
    return ids

def decode_ids(ids: List[int], vocab: Dict[str, int]) -> str:
    """Simple reverse lookup (ignores </w> marker)."""
    inv_vocab = {i: t for t, i in vocab.items()}
    tokens = [inv_vocab[i] for i in ids if i in inv_vocab]
    # Remove the endâ€‘ofâ€‘word marker and concatenate
    text = ''.join(t.replace('</w>', ' ') for t in tokens).strip()
    return text

# --------------------------------------------------------------
# 4ï¸âƒ£  Quick demo
# --------------------------------------------------------------
if __name__ == "__main__":
    training_corpus = [
        "low lowest lower lowly",
        "new newer newest",
        "wide wider widest",
        "quick quickly",
        "hello world"
    ]

    vocab, merges = train_bpe(training_corpus, num_merges=50)
    print("=== Vocabulary (sample) ===")
    for token, idx in list(vocab.items())[:20]:
        print(idx, "â†’", token)

    # Encode a new word
    word = "lowest"
    ids = encode_word(word, merges, vocab)
    print("\nEncoded:", word, "â†’", ids)

    # Decode back
    print("Decoded:", decode_ids(ids, vocab))
```

### What this *schoolâ€‘book* code teaches you

| Step | What you learn |
|------|----------------|
| **Initialisation** | Represent each word as a list of character tuples plus a special endâ€‘ofâ€‘word marker (`</w>`). |
| **Pair counting** | How to count adjacent symbol pairs across the whole corpus. |
| **Merging** | Inâ€‘place replacement of the most frequent pair, building longer subâ€‘words. |
| **Vocabulary construction** | Turning merged tuples into string tokens and assigning integer IDs. |
| **Greedy segmentation** | Reâ€‘applying the same merge order to unseen words. |
| **Decoding** | Simple reverse lookup (useful for debugging). |

> **Caveats** â€“ This implementation is *O(Nâ€¯Â·â€¯M)* where *N* is the number of merges and *M* the corpus size, and it stores the whole corpus in memory. Realâ€‘world tokenizers need a more efficient data structure (e.g. a trie or a hashâ€‘map of pair frequencies) and careful Unicode handling.

---

## 4ï¸âƒ£  Productionâ€‘ready BPE tokenizer

When you move from a teaching demo to a **production system**, you typically want:

| Requirement | Why it matters |
|-------------|----------------|
| **Speed** â€“ tokenisation must be subâ€‘millisecond per sentence (often millions of tokens per second). |
| **Memory efficiency** â€“ the vocab (often 30â€‘50â€¯k entries) should be stored compactly. |
| **Robust Unicode handling** â€“ NFC/NFKC normalisation, emoji, CJK characters, etc. |
| **Deterministic & reproducible** â€“ same input â†’ same token IDs across machines. |
| **Integration with ML frameworks** â€“ easy export to ONNX, TensorFlow, PyTorch, etc. |
| **Support for special tokens** â€“ `<pad>`, `<bos>`, `<eos>`, `<unk>`, etc. |
| **Threadâ€‘safety** â€“ safe to call from many workers (e.g. in a web service). |

The **deâ€‘facto** production solution in the Python ecosystem is the **ğŸ¤—â€¯Huggingâ€¯Face `tokenizers` library** (Rust core, Python bindings). It provides a fast BPE implementation, a clean API, and can be exported to a binary file (`vocab.json` + `merges.txt`) that can be loaded by any framework.

Below is a stepâ€‘byâ€‘step guide to build a productionâ€‘ready BPE tokenizer using that library.

### 4.1  Install the library

```bash
pip install tokenizers   # includes the fast Rust implementation
```

### 4.2  Train a BPE tokenizer on your own data

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, normalizers
from tokenizers.normalizers import NFKC, Strip, Lowercase
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing

# 1ï¸âƒ£  Choose the model (BPE)
bpe_model = models.BPE()

# 2ï¸âƒ£  Define normalisation & preâ€‘tokenisation
normalizer = normalizers.Sequence([NFKC(), Lowercase(), Strip()])
pre_tokenizer = Whitespace()                     # split on whitespace, keep punctuation as separate tokens

# 3ï¸âƒ£  Build the tokenizer object
tokenizer = Tokenizer(bpe_model)
tokenizer.normalizer = normalizer
tokenizer.pre_tokenizer = pre_tokenizer

# 4ï¸âƒ£  Trainer â€“ you can control vocab size, min frequency, special tokens, etc.
trainer = trainers.BpeTrainer(
    vocab_size=30_000,
    min_frequency=2,
    special_tokens=["<pad>", "<s>", "</s>", "<unk>", "<mask>"],
    show_progress=True
)

# 5ï¸âƒ£  Train on a list of files (or a list of strings)
files = ["data/train.txt"]          # each line = a raw sentence
tokenizer.train(files, trainer)

# 6ï¸âƒ£  Postâ€‘processing â€“ add BOS/EOS automatically
tokenizer.post_processor = TemplateProcessing(
    single="<s> $A </s>",
    pair="<s> $A $B </s>",
    special_tokens=[
        ("<s>", tokenizer.token_to_id("<s>")),
        ("</s>", tokenizer.token_to_id("</s>")),
    ],
)

# 7ï¸âƒ£  Save for later reuse (fast loading)
tokenizer.save("bpe_tokenizer.json")
print("Tokenizer saved â€“ vocab size:", tokenizer.get_vocab_size())
```

**What makes this productionâ€‘ready?**

| Feature | Implementation |
|---------|----------------|
| **Fast Rust core** | All tokenisation steps (normalisation, preâ€‘tokenisation, BPE merges) run in compiled Rust â†’ ~10â€‘30Ã— faster than pure Python. |
| **Deterministic** | The same `vocab.json` + `merges.txt` (or the single `.json` above) always yields identical IDs. |
| **Unicodeâ€‘aware** | `NFKC` normalisation handles composed/decomposed characters; the library works on UTFâ€‘8 byte strings directly. |
| **Special tokens** | `<pad>`, `<s>`, `</s>`, `<unk>`, `<mask>` are baked into the vocab and can be referenced by ID. |
| **Threadâ€‘safe** | The tokenizer object can be shared across processes (or reâ€‘loaded per worker). |
| **Exportable** | The JSON file can be loaded by ğŸ¤—â€¯Transformers, ONNX Runtime, or even the Rust `tokenizers` crate in other languages (C++, Java, etc.). |

### 4.3  Using the tokenizer in inference pipelines

```python
from tokenizers import Tokenizer

# Load the saved tokenizer (fast, < 10â€¯ms even for large vocab)
tokenizer = Tokenizer.from_file("bpe_tokenizer.json")

# Encode a single sentence
sentence = "The quick brown fox jumps over the lazy dog."
encoding = tokenizer.encode(sentence)

print("Token IDs :", encoding.ids)
print("Tokens    :", encoding.tokens)

# Decode back (useful for debugging)
print("Decoded   :", tokenizer.decode(encoding.ids))
```

### 4.4  Integration with ğŸ¤—â€¯Transformers (PyTorch / TensorFlow)

If you are already using the `transformers` library, you can directly load the same tokenizer:

```python
from transformers import PreTrainedTokenizerFast

# The file saved by `tokenizer.save` is compatible with this class
tokenizer = PreTrainedTokenizerFast(tokenizer_file="bpe_tokenizer.json",
                                    unk_token="<unk>",
                                    pad_token="<pad>",
                                    bos_token="<s>",
                                    eos_token="</s>")

# Example: tokenising a batch
batch = ["Hello world!", "Byteâ€‘pair encoding is cool."]
enc = tokenizer(batch, padding=True, truncation=True, max_length=64, return_tensors="pt")
print(enc["input_ids"])
print(enc["attention_mask"])
```

Now you can feed `enc["input_ids"]` straight into any `transformers` model (e.g., `GPT2Model`, `BertModel`, etc.) without extra preprocessing.

### 4.5  Production deployment tips

| Tip | Reason |
|-----|--------|
| **Cache the tokenizer** â€“ Load it once at service startâ€‘up, keep the instance in memory. |
| **Avoid perâ€‘request file I/O** â€“ The `.json` file is read only once; subsequent calls are pure inâ€‘memory operations. |
| **Batch tokenisation** â€“ If your API receives many sentences at once, call `tokenizer.batch_encode_plus` (or `tokenizer.encode_batch`) to amortise overhead. |
| **GPUâ€‘friendly tensors** â€“ Return `torch.LongTensor` directly (`return_tensors="pt"`). |
| **Version pinning** â€“ Store the tokenizer file alongside your model checkpoint; this guarantees reproducibility. |
| **Monitoring** â€“ Log the average latency of `tokenizer.encode` in production; if it spikes, check for Unicode edgeâ€‘cases or unusually long inputs. |
| **Security** â€“ The tokenizer does not execute code, but always validate input size (e.g., max characters) to avoid denialâ€‘ofâ€‘service attacks. |

---

## 5ï¸âƒ£  TL;DR Summary

| Concept | Key points |
|---------|------------|
| **Tokenizer** | Turns raw text â†’ token IDs. Handles normalisation, special tokens, unknowns, and can work at word, subâ€‘word, or byte level. |
| **BPE** | Starts with characters, repeatedly merges the most frequent adjacent pair. Produces a compact subâ€‘word vocab that can represent any word by greedy segmentation. |
| **Schoolâ€‘book implementation** | Simple Python code that builds a vocab, merges pairs, and encodes/decodes words. Great for learning but not scalable. |
| **Productionâ€‘ready version** | Use the `tokenizers` library (Rust core) â†’ fast, Unicodeâ€‘aware, threadâ€‘safe, supports special tokens, can be saved/loaded, and integrates with Huggingâ€¯Face Transformers. |


---

Below is a **minimal, humanâ€‘readable example** of the JSON file that the ğŸ¤—â€¯`tokenizers` library writes when you call `tokenizer.save("my_bpe.json")`.  
The file contains **four topâ€‘level sections**:

| Section | What it stores |
|---------|----------------|
| `model` | The actual BPE model â€“ a mapping **token â†’ id** (`vocab`) and the ordered list of **merge operations** (`merges`). |
| `normalizer` | How raw Unicode text is normalised before tokenisation (e.g. NFKC, lowerâ€‘casing, stripping). |
| `pre_tokenizer` | The first split step (usually whitespaceâ€‘based, but can be more complex). |
| `post_processor` | How special tokens like BOS/EOS are added after the BPE step. |

> **Tip:** Only the `model` part is strictly required for a â€œvocabulary JSONâ€.  
> The other sections are optional â€“ if you omit them the tokenizer will fall back to defaults.

---

## 1ï¸âƒ£  Fullâ€‘featured example (â‰ˆâ€¯30â€¯kâ€‘style vocab)

```json
{
  "model": {
    "type": "BPE",
    "vocab": {
      "<pad>": 0,
      "<s>": 1,
      "</s>": 2,
      "<unk>": 3,
      "<mask>": 4,
      "the": 5,
      "â–and": 6,
      "â–to": 7,
      "â–of": 8,
      "â–a": 9,
      "â–in": 10,
      "â–that": 11,
      "â–is": 12,
      "â–it": 13,
      "â–he": 14,
      "â–she": 15,
      "â–was": 16,
      "â–for": 17,
      "â–on": 18,
      "â–with": 19,
      "â–as": 20,
      "â–his": 21,
      "â–her": 22,
      "â–i": 23,
      "â–you": 24,
      "â–we": 25,
      "â–they": 26,
      "â–be": 27,
      "â–at": 28,
      "â–by": 29,
      "â–not": 30,
      "â–from": 31,
      "â–this": 32,
      "â–but": 33,
      "â–or": 34,
      "â–have": 35,
      "â–had": 36,
      "â–were": 37,
      "â–which": 38,
      "â–one": 39,
      "â–all": 40,
      "â–their": 41,
      "â–there": 42,
      "â–when": 43,
      "â–who": 44,
      "â–what": 45,
      "â–so": 46,
      "â–can": 47,
      "â–if": 48,
      "â–would": 49,
      "â–do": 50,
      "â–said": 51,
      "â–about": 52,
      "â–out": 53,
      "â–up": 54,
      "â–more": 55,
      "â–than": 56,
      "â–some": 57,
      "â–into": 58,
      "â–no": 59,
      "â–time": 60,
      "â–just": 61,
      "â–him": 62,
      "â–her": 63,
      "â–my": 64,
      "â–your": 65,
      "â–our": 66,
      "â–their": 67,
      "â–good": 68,
      "â–new": 69,
      "â–first": 70,
      "â–last": 71,
      "â–great": 72,
      "â–little": 73,
      "â–big": 74,
      "â–small": 75,
      "â–old": 76,
      "â–young": 77,
      "â–high": 78,
      "â–low": 79,
      "â–long": 80,
      "â–short": 81,
      "â–right": 82,
      "â–left": 83,
      "â–up": 84,
      "â–down": 85,
      "â–here": 86,
      "â–there": 87,
      "â–where": 88,
      "â–why": 89,
      "â–how": 90,
      "â–because": 91,
      "â–while": 92,
      "â–after": 93,
      "â–before": 94,
      "â–again": 95,
      "â–once": 96,
      "â–twice": 97,
      "â–three": 98,
      "â–four": 99,
      "â–five": 100,
      "â–six": 101,
      "â–seven": 102,
      "â–eight": 103,
      "â–nine": 104,
      "â–ten": 105,
      "â–hundred": 106,
      "â–thousand": 107,
      "â–million": 108,
      "â–billion": 109,
      "â–percent": 110,
      "â–$": 111,
      "â–,": 112,
      "â–.": 113,
      "â–!": 114,
      "â–?": 115,
      "â–'": 116,
      "â–\"": 117,
      "â–(": 118,
      "â–)": 119,
      "â–-": 120,
      "â–/": 121,
      "â–\\": 122,
      "â–:": 123,
      "â–;": 124,
      "â–â€¦": 125,
      "â–ğŸ˜€": 126,
      "â–ğŸš€": 127,
      "â–â¤ï¸": 128,
      "â–##": 129,
      "â–##ing": 130,
      "â–##ed": 131,
      "â–##ly": 132,
      "â–##s": 133,
      "â–##tion": 134,
      "â–##ness": 135,
      "â–##able": 136,
      "â–##ment": 137,
      "â–##ist": 138,
      "â–##er": 139,
      "â–##est": 140,
      "â–##ous": 141,
      "â–##ive": 142,
      "â–##ify": 143,
      "â–##ize": 144,
      "â–##al": 145,
      "â–##ic": 146,
      "â–##ify": 147,
      "â–##tion": 148,
      "â–##ness": 149,
      "â–##hood": 150,
      "â–##ship": 151,
      "â–##less": 152,
      "â–##ful": 153,
      "â–##ward": 154,
      "â–##wise": 155,
      "â–##like": 156,
      "â–##ish": 157,
      "â–##y": 158,
      "â–##en": 159,
      "â–##ify": 160,
      "â–##ate": 161,
      "â–##ify": 162,
      "â–##ise": 163,
      "â–##ise": 164,
      "â–##ise": 165,
      "â–##ise": 166,
      "â–##ise": 167,
      "â–##ise": 168,
      "â–##ise": 169,
      "â–##ise": 170,
      "â–##ise": 171,
      "â–##ise": 172,
      "â–##ise": 173,
      "â–##ise": 174,
      "â–##ise": 175,
      "â–##ise": 176,
      "â–##ise": 177,
      "â–##ise": 178,
      "â–##ise": 179,
      "â–##ise": 180,
      "â–##ise": 181,
      "â–##ise": 182,
      "â–##ise": 183,
      "â–##ise": 184,
      "â–##ise": 185,
      "â–##ise": 186,
      "â–##ise": 187,
      "â–##ise": 188,
      "â–##ise": 189,
      "â–##ise": 190,
      "â–##ise": 191,
      "â–##ise": 192,
      "â–##ise": 193,
      "â–##ise": 194,
      "â–##ise": 195,
      "â–##ise": 196,
      "â–##ise": 197,
      "â–##ise": 198,
      "â–##ise": 199,
      "â–##ise": 200
    },
    "merges": [
      ["â–t", "he"],
      ["â–a", "nd"],
      ["â–t", "o"],
      ["â–o", "f"],
      ["â–i", "n"],
      ["â–w", "as"],
      ["â–h", "e"],
      ["â–s", "he"],
      ["â–h", "er"],
      ["â–i", "t"],
      ["â–b", "e"],
      ["â–a", "t"],
      ["â–f", "or"],
      ["â–o", "n"],
      ["â–w", "ith"],
      ["â–a", "s"],
      ["â–h", "is"],
      ["â–h", "er"],
      ["â–y", "ou"],
      ["â–w", "e"],
      ["â–t", "hey"],
      ["â–b", "ut"],
      ["â–o", "r"],
      ["â–h", "ave"],
      ["â–h", "ad"],
      ["â–w", "ere"],
      ["â–w", "hich"],
      ["â–o", "ne"],
      ["â–a", "ll"],
      ["â–t", "heir"],
      ["â–t", "here"],
      ["â–w", "hen"],
      ["â–w", "ho"],
      ["â–w", "hat"],
      ["â–s", "o"],
      ["â–c", "an"],
      ["â–i", "f"],
      ["â–w", "ould"],
      ["â–d", "o"],
      ["â–s", "aid"],
      ["â–a", "bout"],
      ["â–o", "ut"],
      ["â–u", "p"],
      ["â–m", "ore"],
      ["â–t", "han"],
      ["â–s", "ome"],
      ["â–i", "nto"],
      ["â–n", "o"],
      ["â–t", "ime"],
      ["â–j", "ust"],
      ["â–h", "im"],
      ["â–h", "er"],
      ["â–m", "y"],
      ["â–y", "our"],
      ["â–o", "ur"],
      ["â–g", "ood"],
      ["â–n", "ew"],
      ["â–f", "irst"],
      ["â–l", "ast"],
      ["â–g", "reat"],
      ["â–l", "ittle"],
      ["â–b", "ig"],
      ["â–s", "mall"],
      ["â–o", "ld"],
      ["â–y", "oung"],
      ["â–h", "igh"],
      ["â–l", "ow"],
      ["â–l", "ong"],
      ["â–s", "hort"],
      ["â–r", "ight"],
      ["â–l", "eft"],
      ["â–h", "ere"],
      ["â–t", "here"],
      ["â–w", "here"],
      ["â–w", "hy"],
      ["â–h", "ow"],
      ["â–b", "ecause"],
      ["â–w", "hile"],
      ["â–a", "fter"],
      ["â–b", "efore"],
      ["â–a", "gain"],
      ["â–o", "nce"],
      ["â–t", "wice"],
      ["â–t", "hree"],
      ["â–f", "our"],
      ["â–f", "ive"],
      ["â–s", "ix"],
      ["â–s", "even"],
      ["â–e", "ight"],
      ["â–n", "ine"],
      ["â–t", "en"],
      ["â–h", "undred"],
      ["â–t", "housand"],
      ["â–m", "illion"],
      ["â–b", "illion"],
      ["â–p", "ercent"],
      ["â–$", "$"],
      ["â–,", ","],
      ["â–.", "."],
      ["â–!", "!"],
      ["â–?", "?"],
      ["â–'", "'"],
      ["â–\"", "\""],
      ["â–(", "("],
      ["â–)", ")"],
      ["â–-", "-"],
      ["â–/", "/"],
      ["â–\\", "\\"],
      ["â–:", ":"],
      ["â–;", ";"],
      ["â–â€¦", "â€¦"],
      ["â–ğŸ˜€", "ğŸ˜€"],
      ["â–ğŸš€", "ğŸš€"],
      ["â–â¤ï¸", "â¤ï¸"],
      ["â–##", "##"],
      ["â–##", "ing"],
      ["â–##", "ed"],
      ["â–##", "ly"],
      ["â–##", "s"],
      ["â–##", "tion"],
      ["â–##", "ness"],
      ["â–##", "able"],
      ["â–##", "ment"],
      ["â–##", "ist"],
      ["â–##", "er"],
      ["â–##", "est"],
      ["â–##", "ous"],
      ["â–##", "ive"],
      ["â–##", "ify"],
      ["â–##", "ize"],
      ["â–##", "al"],
      ["â–##", "ic"],
      ["â–##", "hood"],
      ["â–##", "ship"],
      ["â–##", "less"],
      ["â–##", "ful"],
      ["â–##", "ward"],
      ["â–##", "wise"],
      ["â–##", "like"],
      ["â–##", "ish"],
      ["â–##", "y"],
      ["â–##", "en"],
      ["â–##", "ate"]
    ]
  },

  "normalizer": {
    "type": "Sequence",
    "normalizers": [
      { "type": "NFKC" },
      { "type": "Lowercase" },
      { "type": "Strip" }
    ]
  },

  "pre_tokenizer": {
    "type": "Whitespace"
  },

  "post_processor": {
    "type": "TemplateProcessing",
    "single": [
      "<s>", "$A", "</s>"
    ],
    "pair": [
      "<s>", "$A", "$B", "</s>"
    ],
    "special_tokens": [
      { "id": 1, "type_id": 0, "token": "<s>" },
      { "id": 2, "type_id": 0, "token": "</s>" }
    ]
  }
}
```

### What you see in the file

| Part | Example entry | Meaning |
|------|---------------|---------|
| **`vocab`** | `"â–and": 6` | Token string â†’ integer ID. The leading `â–` (U+2581, â€œlower one eighth blockâ€) is the **space marker** that GPTâ€‘2â€‘style byteâ€‘level BPE uses to indicate a preceding whitespace. |
| **`merges`** | `["â–t","he"]` | The first merge operation learned during training. The list order is **exactly** the order the tokenizer will apply when encoding new text. |
| **`normalizer`** | `{"type":"Lowercase"}` | Convert everything to lower case before any splitting. |
| **`pre_tokenizer`** | `{"type":"Whitespace"}` | Split the raw string on whitespace; punctuation stays attached to the preceding token and will later be broken by BPE merges. |
| **`post_processor`** | `"<s>", "$A", "</s>"` | After BPE, prepend BOS (`<s>`) and append EOS (`</s>`) automatically. `$A` is the placeholder for the token list produced by the BPE step. |

---

## 2ï¸âƒ£  Tiny â€œtoyâ€ vocab JSON (easy to read)

If you just want to see the **bare minimum** â€“ a mapping of a handful of tokens and a couple of merges â€“ hereâ€™s a strippedâ€‘down version that you could even write by hand:

```json
{
  "model": {
    "type": "BPE",
    "vocab": {
      "<pad>": 0,
      "<s>": 1,
      "</s>": 2,
      "<unk>": 3,
      "the": 4,
      "â–low": 5,
      "est": 6,
      "â–lowest": 7,
      "â–new": 8,
      "er": 9,
      "est</w>": 10,
      "â–newer": 11,
      "â–newest": 12,
      "â–quick": 13,
      "ly": 14,
      "â–quickly": 15,
      "â–hello": 16,
      "â–world": 17,
      "!</w>": 18
    },
    "merges": [
      ["â–", "low"],
      ["low", "est"],
      ["â–", "new"],
      ["new", "er"],
      ["new", "est"],
      ["â–", "quick"],
      ["quick", "ly"],
      ["â–", "hello"],
      ["â–", "world"],
      ["world", "!"]
    ]
  },

  "normalizer": {
    "type": "Sequence",
    "normalizers": [
      { "type": "NFKC" },
      { "type": "Lowercase" }
    ]
  },

  "pre_tokenizer": { "type": "Whitespace" },

  "post_processor": {
    "type": "TemplateProcessing",
    "single": ["<s>", "$A", "</s>"],
    "pair": ["<s>", "$A", "$B", "</s>"],
    "special_tokens": [
      { "id": 1, "type_id": 0, "token": "<s>" },
      { "id": 2, "type_id": 0, "token": "</s>" }
    ]
  }
}
```

*Running this tiny tokenizer on the sentence*  

```text
"The lowest new quickly hello world!"
```  

*produces the following IDs* (using the IDs from the `vocab` above):

```
[1, 4, 7, 11, 15, 16, 17, 18, 2]
#   ^   ^   ^   ^   ^   ^   ^   ^   ^
# <s> the lowest newer quickly hello world! </s>
```

---

## 3ï¸âƒ£  How to load the JSON in code

```python
from tokenizers import Tokenizer

# Load the file you just saved (fast Rust implementation)
tokenizer = Tokenizer.from_file("my_bpe.json")

# Encode a batch (the postâ€‘processor automatically adds BOS/EOS)
enc = tokenizer.encode("The lowest new quickly hello world!")
print("ids :", enc.ids)          # â†’ [1, 4, 7, 11, 15, 16, 17, 18, 2]
print("tokens :", enc.tokens)    # â†’ ['<s>', 'the', 'â–lowest', 'â–new', 'â–quickly', 'â–hello', 'â–world', '!</w>', '</s>']
```

Thatâ€™s it â€“ the JSON file is the **single source of truth** for both training and inference.  
When you ship a model, just bundle `my_bpe.json` together with the checkpoint; any environment that can import `tokenizers` (Python, Rust, C++, Java, Node, â€¦) will be able to reproduce exactly the same token IDs.

---

### TL;DR

* The **vocabulary JSON** contains a `model` block with a `vocab` dictionary (`token â†’ id`) and a `merges` list (ordered BPE merges).  
* It also stores optional `normalizer`, `pre_tokenizer`, and `post_processor` sections so the whole tokenisation pipeline can be reconstructed from a single file.  
* A tiny example (â‰ˆâ€¯20 tokens) is shown above, and a more realistic â€œ30â€¯kâ€‘styleâ€ snippet demonstrates the exact structure youâ€™ll see in production.  

Feel free to copy the tiny example, edit the token strings, and run `Tokenizer.from_file` to see it in action!


---

A key optimization that separates a simple educational implementation from a high-performance, production-ready one is optimizing the use of merges.

Using a Trie (or a similar optimized structure) is crucial for making the **encoding** step fast and efficient.

Let's break down where a Trie would fit and what its benefits are.

### The Core Problem with the Naive `encode` Function

First, let's look at the bottleneck in the school-book `encode_word` function:

```python
def encode_word(word: str, merges: List[...], vocab: Dict[...]) -> List[int]:
    # ...
    # Apply merges in the same order as training
    for merge in merges:  # This loop is the problem!
        i = 0
        while i < len(symbols) - 1:
            if (symbols[i], symbols[i + 1]) == merge:
                # ... merge and delete ...
            else:
                i += 1
    # ...
```

For every single word we want to encode, this function iterates through the **entire list of merge rules** (which could be thousands long). If `num_merges` is 30,000, this is a huge amount of repeated work.

This is where a Trie provides a much more elegant and performant solution.

### How a Trie Solves the Encoding Problem

Instead of re-applying the merge rules, we can use the **final vocabulary** to build a Trie. Each path from the root to a node in the Trie represents a valid token.

1.  **Build the Vocabulary Trie:**
    Take all the tokens from your final BPE vocabulary (e.g., `l`, `o`, `w`, `lo`, `es`, `wes`, `lowes`, `lowest`, `</w>`) and insert them into a Trie.

    A simplified view of the Trie would look like this:

    ```
         (root)
         /  |  \
        l   w   <
       /   / \   \
      o   e   i   /
     /   / \   \   \
    w   s   d   w   >
     \   \   \   \
      e   t   e   >
       \       \
        s       r
         \
          t
    ```
    *(Each path from the root, like `l`->`o`->`w`->`e`->`s`->`t`, represents a token: "lowest")*

2.  **Greedy Longest-Match Encoding:**
    Now, to tokenize a new word like `"lowest</w>"`, you perform a **greedy longest-prefix match** against this Trie.

    *   **Start at index 0 (`l`).** Traverse the Trie: `l` is a token. `lo` is a token. `low` is not. `lowe` is not. `lowes` is a token. `lowest` is a token. `lowest<` is not.
    *   The longest possible token starting at index 0 is `"lowest"`.
    *   **Result:** Emit the token `"lowest"`. Advance your position in the word by `len("lowest")`.
    *   **Continue from the new position.** In this case, we have `"</w>"` left. The longest match is `"</w>"`.
    *   **Result:** Emit the token `"</w>"`.
    *   **Final tokens:** `["lowest", "</w>"]`.

This process requires only a **single pass** over the input word, making it dramatically faster than the naive loop-over-merges approach.

---

### What about the **Training** Step?

Using a Trie for the training part is less common and more complex. The main bottleneck during training is re-calculating pair frequencies (`get_pair_frequencies`) after every merge. Production tokenizers solve this with a different optimization:

*   Instead of re-scanning the whole corpus, they maintain an index of where each pair occurs.
*   When a pair `(A, B)` is merged into `C`, they only need to update the counts for the pairs immediately adjacent to the merge points. For example, if you had `... X A B Y ...`, you would:
    *   Decrement the count of `(X, A)` and `(B, Y)`.
    *   Increment the count of `(X, C)` and `(C, Y)`.
*   This local update is much faster than a global rescan. This is often managed with a combination of linked lists (to represent the token sequences) and a priority queue (to store the pair frequencies and quickly find the max).

So, the optimizations are typically split:
1.  **Training:** Efficient pair counting using indexed/linked structures and a priority queue.
2.  **Encoding:** A Trie built from the final vocabulary for fast longest-match segmentation.

### Summary: Trie Benefits vs. School-Book Hash Map

| Aspect | School-Book (Hash Map / Dict) | Production (Trie-based Encoding) |
| :--- | :--- | :--- |
| **Data Structure** | `merges`: A `List` of merge rules.<br>`vocab`: A `Dict` mapping string -> ID. | `vocab`: A **Trie** where each path is a valid token. |
| **Encoding Algorithm** | **Re-apply all merge rules** sequentially for each new word. | **Greedy longest-prefix match** against the Trie in a single pass over the word. |
| **Time Complexity (Encoding)**| *O(len(word) * num_merges)* | *O(len(word))*, as Trie lookups are proportional to token length, not vocabulary size. |
| **Benefit** | **Simple to understand and implement.** Clearly demonstrates the BPE merge logic. | **Extremely fast.** This is the method used in libraries like `sentencepiece` and Hugging Face `tokenizers`. |
| **Drawback** | **Very slow.** Unsuitable for any real-world application. | **More complex to implement.** The Trie data structure itself is more involved than a simple list or dictionary. |

In conclusion, your intuition is spot on. The move from a hash-map-based "replay the merges" strategy to a **Trie-based longest-match strategy** is a fundamental step in building a tokenizer that is not just correct, but also performant enough for production systems.